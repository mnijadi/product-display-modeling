# Product Display Analysis and Modeling

## Description

Given a dataset of product attributes in different stores, our job is to perform a statistical analysis on the data to discover unerlying patterns, discover different preprocessing techniques, and finally, apply machine learning models to predict the display of a product in a store.

## Dataset

A single csv file contained in the `data\raw` folder.

## Tools

We are developing our models in a jupyter notebook environment, using Python and it's libraries:

- Numpy / Pandas / Matplotlib / Seaborn
- Scikit-learn
- XGBoost
- CatBoost
- Keras
- MLFlow
- Flask

## Setup

### Development and Experiment Tracking

Create a Pytohn virtual environment:

```{bash}
pip install notebook numpy statsmodels matplotlib seaborn pandas scikit-learn mlxtend xgboost catboost tensorflow mlflow
```

```{bash}
python -m venv .venv
source .venv/bin/activate
pip install requirements.txt
```

Open the notebooks in the `notebooks` folder, following the order of the numbers in the file names.  
Each notebook transforms the data such that it can be used in the next notebook.  
The final notebook contains the different models and experiments we have tried.
Before running the final notebook:

```{bash}
mlflow server --host 127.0.0.1 --port 8080
```

This will start the MLFlow server, which will be used to track the different experiments and models.  
After running the final notebook, you can view the results in the MLFlow UI at `http://127.0.0.1:8080/`, all the experiments and models will be logged there.

### Deployment

The last notebook will export the best model's run ID as an environment variable, which will be used in the deployment (As this parameter is randomely generated by MLFlow).

```{bash}
cd src
python predict.py
```

This will start a Flask server, which will be used to serve the model locally.

## Wrap-up

During this project, here are the different steps we have followed:

- Data analysis through statistics, hypothesis testing, and visualizations.
- Data preprocessing with one hot encoding and feature engineering using discretization techniques.
- Machine learning modeling using different algorithms and libraries and hyperparameter tuning.
- Experiment tracking using MLFlow.
- Model deployment using REST API with Flask.

The next steps would be to:

- Containerize the application using Docker.
- Deploy the application to a cloud service.
- Track experiments and models on a cloud database instead of the local file system.
- Monitor the chosen model, eventhough we expect the performance to remain the same as there is no new data coming in.
